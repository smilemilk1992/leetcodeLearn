# 1、HashMap简介 JDK1.8 https://zhuanlan.zhihu.com/p/125628540 https://www.jianshu.com/p/4aa3bb16f36c
* HashMap是基于哈希表实现的，每一个元素是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。

* HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。

* HashMap 实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。

* HashMap存数据的过程是：

* HashMap内部维护了一个存储数据的Entry数组，HashMap采用链表解决冲突，每一个Entry本质上是一个单向链表。当准备添加一个key-value对时，首先通过hash(key)方法计算hash值，然后通过indexFor(hash,length)求该key-value对的存储位置，计算方法是先用hash&0x7FFFFFFF后，再对length取模，这就保证每一个key-value对都能存入HashMap中，当计算出的位置相同时，由于存入位置是一个链表，则把这个key-value对插入链表头。

* HashMap中key和value都允许为null。key为null的键值对永远都放在以table[0]为头结点的链表中。

* HashMap继承自AbstractMap类。并且实现了Map接口

* HashMap底层是一个Entry数组，当发生hash冲突的时候，hashmap是采用链表的方式来解决的，在对应的数组位置存放链表的头结点。对链表而言，新加入的节点会从头结点加入。

        // 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。      
            void addEntry(int hash, K key, V value, int bucketIndex) {      
                // 保存“bucketIndex”位置的值到“e”中      
                Entry<K,V> e = table[bucketIndex];      
                // 设置“bucketIndex”位置的元素为“新Entry”，      
                // 设置“e”为“新Entry的下一个节点”      
                table[bucketIndex] = new Entry<K,V>(hash, key, value, e);      
                // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小      
                if (size++ >= threshold)      
                    resize(2 * table.length);      
            }  
*在hashmap做put操作的时候会调用到以上的方法。现在假如A线程和B线程同时对同一个数组位置调用addEntry，两个线程会同时得到现在的头结点，然后A写入新的头结点之后，B也写入新的头结点，那B的写入操作就会覆盖A的写入操作造成A的写入操作丢失

* HashMap把Hashtable的contains方法去掉了，改成containsValue和containsKey，因为contains方法容易让人引起误解。
* hash函数是先拿到通过key 的hashcode，是32位的int值，然后让hashcode的高16位和低16位进行异或操作。https://www.it610.com/article/1280526439128514560.htm

        static final int hash(Object key) {
                int h;
                return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
            }
            异或运算能更好的保留各部分的特征，如果采用&运算计算出来的值会向1靠拢，采用|运算计算出来的值会向0靠拢
            主要是从速度，功效和质量来考虑的，减少系统的开销，也不会造成因为高位没有参与下标的计算，从而引起的碰撞。
            最终目的还是为了让哈希后的结果更均匀的分部，减少哈希碰撞，提升hashmap的运行效率
            https://www.cnblogs.com/zxporz/p/11204233.html
* 底层数组+链表或红黑树实现，可以存储null键和null值，线程不安全
* 初始size为16，扩容：newsize = oldsize*2，size一定为2的n次幂
* 扩容针对整个Map，每次扩容时，原来数组中的元素依次重新计算存放位置，并重新插入
* 插入元素后才判断该不该扩容，有可能无效扩容（插入后如果扩容，如果没有再次插入，就会产生无效扩容）
* 当Map中元素总数超过Entry数组的75%，触发扩容操作，为了减少链表长度，元素分配更均匀
* 计算index方法：index = hash & (tab.length – 1)
* 链表转红黑树阈值是8以空间换时间，这样以来查询的效率就变为O(logN)  红黑树转链表阈值是6
* 内部节点插入无序，有序的Map是LinkedHashMap 和 TreeMap
* LinkedHashMap内部维护了一个单链表，有头尾节点，同时LinkedHashMap节点Entry内部除了继承HashMap的Node属性，还有before 和 after用于标识前置节点和后置节点。可以实现按插入的顺序或访问顺序排序。
* HashMap的容量必须是2的N次方，如果你指定了一个非2的N次方的整数S，那么HashMap在内部会把它转化为大于S的2的N次方的整数。
* HashMap在并发执行put操作时会引起死循环，是因为多线程会导致HashMap的Entry链表 形成环形数据结构，一旦形成环形数据结构，Entry的next节点永远不为空，就会产生死循环获 取Entry。

HashMap的初始值还要考虑加载因子:
* 哈希冲突：若干Key的哈希值按数组大小取模后，如果落在同一个数组下标上，将组成一条Entry链，对Key的查找需要遍历Entry链上的每个元素执行equals()比较。
* 加载因子：为了降低哈希冲突的概率，默认当HashMap中的键值对达到数组大小的75%时，即会触发扩容。因此，如果预估容量是100，即需要设定100/0.75＝134的数组大小。
* 空间换时间：如果希望加快Key查找的时间，还可以进一步降低加载因子，加大初始大小，以降低哈希冲突的概率。


1.7 与1.8区别

* 数组+链表改成了数组+链表或红黑树；
* 链表的插入方式从头插法改成了尾插法，简单说就是插入时，如果数组位置上已经有元素，1.7将新元素放到数组中，原始节点作为新节点的后继节点，1.8遍历链表，将元素放置到链表的最后；
* 扩容的时候1.7需要对原数组中的元素进行重新hash定位在新数组的位置，1.8采用更简单的判断逻辑，位置不变或索引+旧容量大小；
* 在插入时，1.7先判断是否需要扩容，再插入，1.8先进行插入，插入完成再判断是否需要扩容；

hash冲突-https://www.cnblogs.com/peizhe123/p/5790252.html
如果两个不同的元素，通过哈希函数得出的实际存储地址相同怎么办？也就是说，当我们对某个元素进行哈希运算，得到一个存储地址，然后要进行插入的时候，发现已经被其他元素占用了，其实这就是所谓的哈希冲突，也叫哈希碰撞。前面我们提到过，哈希函数的设计至关重要，好的哈希函数会尽可能地保证 计算简单和散列地址分布均匀,但是，我们需要清楚的是，数组是一块连续的固定长度的内存空间，再好的哈希函数也不能保证得到的存储地址绝对不发生冲突。那么哈希冲突如何解决呢？哈希冲突的解决方案有多种:开放定址法（发生冲突，继续寻找下一块未被占用的存储地址），再散列函数法，链地址法，而HashMap即是采用了链地址法，也就是数组+链表的方式

https://www.cnblogs.com/duodushuduokanbao/p/9492952.html
HashMap的put方法实现
 思路如下：

1、判断当前桶是否为空，空的就需要初始化（resize 中会判断是否进行初始化）。

2、根据当前 key 的 hashcode 定位到具体的桶中并判断是否为空，为空表明没有 Hash 冲突就直接在当前位置创建一个新桶即可。

3、如果当前桶有值（ Hash 冲突），那么就要比较当前桶中的 key、key 的 hashcode 与写入的 key 是否相等，相等就赋值给 e,在第 8 步的时候会统一进行赋值及返回。

4、如果当前桶为红黑树，那就要按照红黑树的方式写入数据。

5、如果是个链表，就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面（形成链表）。

6、接着判断当前链表的大小是否大于预设的阈值，大于8时就要转换为红黑树。

7、如果在遍历过程中找到 key 相同时直接退出遍历。

8、如果 e != null 就相当于存在相同的 key,那就需要将值覆盖。

9、最后判断是否需要进行扩容。


HashMap的get方法实现
 实现思路：
1、首先将 key hash 之后取得所定位的桶。

2、如果桶为空则直接返回 null 。

3、否则判断桶的第一个位置(有可能是链表、红黑树)的 key 是否为查询的 key，是就直接返回 value。

4、如果第一个不匹配，则判断它的下一个是红黑树还是链表。

5、红黑树就按照树的查找方式返回值。

6、不然就按照链表的方式遍历匹配返回值。

https://www.cnblogs.com/duodushuduokanbao/p/9492952.html

无论是 1.7 还是 1.8 其实都能看出 JDK 没有对它做任何的同步操作，所以并发会出问题，甚至出现死循环导致系统不可用。


HashMap在jdk1.8之后引入了红黑树的概念，表示若桶中链表元素超过8时（能触发转化的两个条件是：一个是链表的长度达到8个，一个是数组的长度达到64个（当满足条件1以后调用treeifyBin方法转化红黑树。该方法中，数组如果长度小于MIN_TREEIFY_CAPACITY（64）就选择扩容，而不是转化为红黑树。）），会自动转化成红黑树；若桶中元素小于等于6时，树结构还原成链表形式。
（所以当链表元素数目到8个，同时HashMap的数组长度要大于64，链表才会转红黑树，否则都是做扩容。）

原因：

红黑树的平均查找长度是log(n)，长度为8，查找长度为log(8)=3，链表的平均查找长度为n/2，当长度为8时，平均查找长度为8/2=4，这才有转换成树的必要；链表长度如果是小于等于6，6/2=3，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短。

还有选择6和8的原因是：

中间有个差值7可以防止链表和树之间频繁的转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。


　从putVal源代码中我们可以知道，当插入一个元素的时候size就加1，若size大于threshold的时候，就会进行扩容。假设我们的capacity大小为32，loadFator为0.75,则threshold为24 = 32 * 0.75，此时，插入了25个元素，并且插入的这25个元素都在同一个桶中，桶中的数据结构为红黑树，则还有31个桶是空的，也会进行扩容处理，其实，此时，还有31个桶是空的，好像似乎不需要进行扩容处理，但是是需要扩容处理的，因为此时我们的capacity大小可能不适当。我们前面知道，扩容处理会遍历所有的元素，时间复杂度很高；前面我们还知道，经过一次扩容处理后，元素会更加均匀的分布在各个桶中，会提升访问效率。所以，说尽量避免进行扩容处理，也就意味着，遍历元素所带来的坏处大于元素在桶中均匀分布所带来的好处。如果有读者有不同意见，也欢迎讨论~



当负载因子过大时，Hash冲突概率增加、读写的时间复杂度也大大增加，当负载因子过小时，Hash冲突概率较小，时间复杂度较低，但占用内存空间较大。